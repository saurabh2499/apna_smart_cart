{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "#import time\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_applications import resnet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from model import SSD300v2\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop(img):\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     th, threshed = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "#     kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "#     morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\n",
    "#     cnts = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "#     cnt = sorted(cnts, key=cv2.contourArea)[-1]\n",
    "#     x,y,w,h = cv2.boundingRect(cnt)\n",
    "#     dst = img[y:y+h, x:x+w]\n",
    "#     return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func(image):\n",
    "    \n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "#     thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "#     thresh = cv2.erode(thresh, None, iterations=2)\n",
    "#     thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "#     cnts,_ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     areas = [cv2.contourArea(c) for c in cnts]\n",
    "#     max_index = np.argmax(areas)\n",
    "#     c=cnts[max_index]\n",
    "\n",
    "#     M=cv2.moments(c)\n",
    "\n",
    "#     extTop=int(M['m10']/M['m00'])\n",
    "#     extBot=int(M['m01']/M['m00'])\n",
    "#     ext=(extTop,extBot)\n",
    "\n",
    " \n",
    "\n",
    "#     cv2.waitKey(0)\n",
    "#     return extBot\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = 'E:\\\\source\\\\practice\\\\hackfest\\\\detect_model\\\\detect_model\\\\data'\n",
    "# y = []\n",
    "# images = []\n",
    "# for filename in os.listdir(folder):\n",
    "#     a = list(filename.split(\".\"))\n",
    "#     y.append(int(a[0]))\n",
    "#     img = cv2.imread(os.path.join(folder,filename), cv2.IMREAD_COLOR)\n",
    "#     if img is not None:\n",
    "#         img = cv2.resize(img, (300, 300))\n",
    "#         images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'E:\\\\detect_model\\\\data1\\\\'\n",
    "subfolders = [f.path for f in os.scandir(folder) if f.is_dir() ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "objects = ['hand', 'bottle', 'disk','snacks']\n",
    "i = 0\n",
    "y = []\n",
    "for folder in subfolders:\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        a = list(filename.split(\".\"))\n",
    "        #y.append(int(a[0]))\n",
    "        img = cv2.imread(os.path.join(folder,filename), cv2.IMREAD_COLOR)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (10, 10))\n",
    "            images.append(img)\n",
    "            y.append(i)\n",
    "            \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = to_categorical(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ = SSD300v2(images[0].shape)\n",
    "# model_.summary()\n",
    "# model_.compile(optimizer='Adam', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.resize(images[0], (300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = im.reshape(1, 300, 300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2287, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#y = to_categorical(y)\n",
    "#y = np.array(y)\n",
    "images = np.array(images)\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, k, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 10,10, 3).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 10, 10, 3).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1829, 10, 10, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(10, 10, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = larger_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 32)          2432      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 85,860\n",
      "Trainable params: 85,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1829 samples, validate on 458 samples\n",
      "Epoch 1/32\n",
      "1829/1829 [==============================] - 1s 414us/step - loss: 0.9700 - acc: 0.6020 - val_loss: 0.3602 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.60197, saving model to weights-improvement-latest-age-01-0.94.hdf5\n",
      "Epoch 2/32\n",
      "1829/1829 [==============================] - 0s 239us/step - loss: 0.2341 - acc: 0.9317 - val_loss: 0.1457 - val_acc: 0.9520\n",
      "\n",
      "Epoch 00002: acc improved from 0.60197 to 0.93166, saving model to weights-improvement-latest-age-02-0.95.hdf5\n",
      "Epoch 3/32\n",
      "1829/1829 [==============================] - 0s 260us/step - loss: 0.1746 - acc: 0.9382 - val_loss: 0.0325 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00003: acc improved from 0.93166 to 0.93822, saving model to weights-improvement-latest-age-03-0.99.hdf5\n",
      "Epoch 4/32\n",
      "1829/1829 [==============================] - 0s 244us/step - loss: 0.0514 - acc: 0.9847 - val_loss: 0.0223 - val_acc: 0.9978\n",
      "\n",
      "Epoch 00004: acc improved from 0.93822 to 0.98469, saving model to weights-improvement-latest-age-04-1.00.hdf5\n",
      "Epoch 5/32\n",
      "1829/1829 [==============================] - 0s 226us/step - loss: 0.0332 - acc: 0.9891 - val_loss: 0.0076 - val_acc: 0.9978\n",
      "\n",
      "Epoch 00005: acc improved from 0.98469 to 0.98907, saving model to weights-improvement-latest-age-05-1.00.hdf5\n",
      "Epoch 6/32\n",
      "1829/1829 [==============================] - 0s 226us/step - loss: 0.0207 - acc: 0.9956 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00006: acc improved from 0.98907 to 0.99563, saving model to weights-improvement-latest-age-06-1.00.hdf5\n",
      "Epoch 7/32\n",
      "1829/1829 [==============================] - 0s 234us/step - loss: 0.0328 - acc: 0.9902 - val_loss: 0.0312 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00007: acc did not improve from 0.99563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    eas = keras.callbacks.EarlyStopping(monitor='acc', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "    tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "    filepath=\"weights-improvement-latest-age-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint, tbCallBack, eas]\n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test),callbacks= callbacks_list, epochs=32, batch_size=32)\n",
    "    model.save('total1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458/458 [==============================] - 0s 624us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0312004669784979, 0.9868995627982127]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('total1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('gy.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"GlhoxYgeZ7pFrSCGjMaP\":\"box\",\"N4tDJgWSXGpyKfowqgZz\":\"blue_bottle\",\"RkPATLS2uy2eSHNNjsiP\":\"notebook\",\"TU8yf0fRdSS8fnBkLlCr\":\"cocacola_can\",\"rYeFRV3sXrjyfdNeaSUD\":\"red_bottle\",\"tS9KRyw5wnnlinxrwK2g\":\"box_cap\"}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ast.literal_eval(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GlhoxYgeZ7pFrSCGjMaP': 'box',\n",
       " 'N4tDJgWSXGpyKfowqgZz': 'blue_bottle',\n",
       " 'RkPATLS2uy2eSHNNjsiP': 'notebook',\n",
       " 'TU8yf0fRdSS8fnBkLlCr': 'cocacola_can',\n",
       " 'rYeFRV3sXrjyfdNeaSUD': 'red_bottle',\n",
       " 'tS9KRyw5wnnlinxrwK2g': 'box_cap'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {v: k for k, v in x.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'box': 'GlhoxYgeZ7pFrSCGjMaP',\n",
       " 'blue_bottle': 'N4tDJgWSXGpyKfowqgZz',\n",
       " 'notebook': 'RkPATLS2uy2eSHNNjsiP',\n",
       " 'cocacola_can': 'TU8yf0fRdSS8fnBkLlCr',\n",
       " 'red_bottle': 'rYeFRV3sXrjyfdNeaSUD',\n",
       " 'box_cap': 'tS9KRyw5wnnlinxrwK2g'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4284, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4284, 300, 300, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"total1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17aeeb92ba8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.resize(images[k], (10, 10))\n",
    "im = im.reshape(1, 10, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = model.predict(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(P))\n",
    "print(y[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
